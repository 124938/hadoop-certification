sqoop help

/******************************************************************************
usage: sqoop COMMAND [ARGS]

Available commands:
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  eval               Evaluate a SQL statement and display the results
  export             Export an HDFS directory to a database table
  help               List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  import-mainframe   Import datasets from a mainframe server to HDFS
  job                Work with saved jobs
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  merge              Merge results of incremental imports
  metastore          Run a standalone Sqoop metastore
  version            Display version information
******************************************************************************/

/********************************* Common useful commands - START **********************************/

==========================================
#### Problem Statement - Display version
==========================================

sqoop version

==============================================================
#### Problem Statement - List available databases on a server
==============================================================

sqoop list-databases \
 --connect jdbc:mysql://quickstart.cloudera:3306 \
 --username retail_dba \
 --password cloudera

=============================================================
#### Problem Statement - List available tables on a database
=============================================================

sqoop list-tables \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera

====================================================================================
#### Problem Statement - Evaluate a SQL statement (DDL/DML) and display the results
====================================================================================

sqoop eval \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --query "select count(1) from customers"

/********************************* Common useful commands - END **********************************/

/********************************* Import all tables - START **********************************/

sqoop import-all-tables --help

================================================================
#### Problem Statement - Import all tables from database to HDFS
================================================================

hadoop fs -mkdir /user/cloudera/sqoop_import_all_retail_db_text

sqoop import-all-tables \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --warehouse-dir /user/cloudera/sqoop_import_all_retail_db_text

hadoop fs -ls -R /user/cloudera/sqoop_import_all_retail_db_text
hadoop fs -cat /user/cloudera/sqoop_import_all_retail_db_text/categories/part* | wc -l
hadoop fs -cat /user/cloudera/sqoop_import_all_retail_db_text/orders/part* | wc -l

mysql -u retail_dba -p
mysql> use retail_db;
mysql> select count(1) from categories;
mysql> select count(1) from orders;

======================================================================================================
#### Problem Statement - Import all tables from database to HDFS (By changing the default delimiters)
======================================================================================================

hadoop fs -mkdir /user/cloudera/sqoop_import_all_retail_db_text_delimiter

sqoop import-all-tables \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --warehouse-dir /user/cloudera/sqoop_import_all_retail_db_text_delimiter \
 --enclosed-by "\"" \
 --fields-terminated-by "|" \
 --lines-terminated-by "\n"

hadoop fs -ls -R /user/cloudera/sqoop_import_all_retail_db_text_delimiter
hadoop fs -cat /user/cloudera/sqoop_import_all_retail_db_text_delimiter/categories/part*

==================================================================================================
#### Problem Statement - Import all tables from database to HDFS (By compressing the text file)
==================================================================================================

hadoop fs -mkdir /user/cloudera/sqoop_import_all_retail_db_text_comp

sqoop import-all-tables \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --warehouse-dir /user/cloudera/sqoop_import_all_retail_db_text_comp \
 --as-textfile \
 --compress \
 --compression-codec org.apache.hadoop.io.compress.SnappyCodec

hadoop fs -ls -R /user/cloudera/sqoop_import_all_retail_db_text_comp
hadoop fs -cat /user/cloudera/sqoop_import_all_retail_db_text_comp/categories/part*

=============================================================================================================================
#### Problem Statement - Import all tables from database to HDFS (By changing the file format as AVRO with file compression)
=============================================================================================================================

hadoop fs -mkdir /user/cloudera/sqoop_import_all_retail_db_avro_comp

sqoop import-all-tables \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --warehouse-dir /user/cloudera/sqoop_import_all_retail_db_avro_comp \
 --as-avrodatafile \
 --compress \
 --compression-codec org.apache.hadoop.io.compress.SnappyCodec

hadoop fs -ls -R /user/cloudera/sqoop_import_all_retail_db_avro_comp

avro-tools tojson hdfs://quickstart.cloudera/user/cloudera/sqoop_import_all_retail_db_avro_comp/categories/part-m-00000.avro
avro-tools tojson hdfs://quickstart.cloudera/user/cloudera/sqoop_import_all_retail_db_avro_comp/customers/part-m-00000.avro

=================================================================================================================
#### Problem Statement - Import all tables from database to HDFS (By changing the file format as Sequence file)
=================================================================================================================

hadoop fs -mkdir /user/cloudera/sqoop_import_all_retail_db_seq

sqoop import-all-tables \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --warehouse-dir /user/cloudera/sqoop_import_all_retail_db_seq \
 --as-sequencefile 

hadoop fs -ls -R /user/cloudera/sqoop_import_all_retail_db_seq

=================================================================================================================
#### Problem Statement - Import all tables from database to HDFS (By changing the file format as Parquet file)
=================================================================================================================

hadoop fs -mkdir /user/cloudera/sqoop_import_all_retail_db_parquet

sqoop import-all-tables \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --warehouse-dir /user/cloudera/sqoop_import_all_retail_db_parquet \
 --as-parquetfile 

hadoop fs -ls -R /user/cloudera/sqoop_import_all_retail_db_parquet

parquet-tools --help
parquet-tools cat --json hdfs://quickstart.cloudera/user/cloudera/sqoop_import_all_retail_db_parquet/products/a09a312d-7bd2-4049-a5eb-8f3a41c3daa1.parquet
parquet-tools head --records 10 hdfs://quickstart.cloudera/user/cloudera/sqoop_import_all_retail_db_parquet/products/a09a312d-7bd2-4049-a5eb-8f3a41c3daa1.parquet
parquet-tools schema --detailed hdfs://quickstart.cloudera/user/cloudera/sqoop_import_all_retail_db_parquet/products/a09a312d-7bd2-4049-a5eb-8f3a41c3daa1.parquet

/********************************* Import all tables - END **********************************/


/********************************* Import all tables in HIVE - START **********************************/

===================================================================================================================
#### Problem Statement - Import all tables from database to HIVE database (By creating hive tables in text format)
===================================================================================================================

hive
hive> create database Sqoop_import_all_retail_db_text;
hive> use sqoop_import_all_retail_db_text;
hive> show tables;

sqoop import-all-tables \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --warehouse-dir /user/cloudera/sqoop_import_all_retail_db_text_hive \
 --hive-import \
 --hive-home /user/hive/warehouse \
 --hive-database sqoop_import_all_retail_db_text \
 --hive-overwrite \
 --create-hive-table

hadoop fs -ls -R /user/hive/warehouse/sqoop_import_all_retail_db_text.db

hive
hive> use sqoop_import_all_retail_db_text;
hive> show tables;
hive> select count(1) from categories;
hive> select count(1) from products;

Note:
-> Only text file is getting supported while importing all tables to hive, following options are not supported in above mentioned sqoop command
--as-sequencefile
--as-avrodatafile
--as-parquetfile 

/********************************* Import all tables in HIVE - END **********************************/


/********************************* Import specific table - START **********************************/

hadoop fs -mkdir /user/cloudera/sqoop_import_retail_db

========================================================================================
#### Problem Statement - Import "departments" table from MySQL to HDFS (Default import)
========================================================================================

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table departments \
 --target-dir /user/cloudera/sqoop_import_retail_db/departments

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/departments
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/departments/part*

Note:
-> --num-mappers default value is 4

==================================================================================================================
#### Problem Statement - Import "customers" table from MySQL to HDFS (By using boundary query and columns option)
==================================================================================================================

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table customers \
 --columns "customer_id, customer_fname, customer_lname, customer_email, customer_password, customer_street, customer_city, customer_state, customer_zipcode" \
 --boundary-query "select min(customer_id), max(customer_id) from customers" \
 --target-dir /user/cloudera/sqoop_import_retail_db/customers \
 --num-mappers 1 

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/customers
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/customers/part*

========================================================================================================
#### Problem Statement - Import "departments_nopk" table from MySQL to HDFS (By using split by option)
========================================================================================================

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> use retail_db;
mysql> create table departments_nopk as select * from departments;
mysql> describe departments_nopk;

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table departments_nopk \
 --split-by department_id \
 --target-dir /user/cloudera/sqoop_import_retail_db/departments_nopk

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/departments_nopk
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/departments_nopk/part*

Note:
-> --split-by option is useful in case of table doesn't have primary key
-> --num-mappers=1 can be use in absense on --split-by option

==========================================================================================================
#### Problem Statement - Import data using query from MySQL to HDFS (By using query and splity by option)
==========================================================================================================

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --query "select * from orders join order_items on (orders.order_id = order_items.order_item_order_id) where \$CONDITIONS" \
 --split-by order_id \
 --target-dir /user/cloudera/sqoop_import_retail_db/order_join

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/order_join
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/order_join/part*

Note:
-> --table and --query options are mutually exclusive
-> where \$CONDITIONS is mandatory in case of using --query option
-> --split-by option is useful in case of using --query option
-> --num-mappers=1 can be use in absense on --split-by option

=======================================================================================================================
#### Problem Statement - Import "categories" table incrementally from MySQL to HDFS (By using where and append clause)
=======================================================================================================================

-------------
First time :
-------------

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table categories \
 --where "category_id > 0" \
 --append \
 --target-dir /user/cloudera/sqoop_import_retail_db/categories

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/categories
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/categories/part*

----------------------------------
Insert new records into MySQL DB :
----------------------------------

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> use retail_db;
mysql> insert into categories values (59, 7, 'testing 59');
mysql> insert into categories values (60, 7, 'testing 60');

-------------
Second time :
-------------

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table categories \
 --where "category_id > 58" \
 --target-dir /user/cloudera/sqoop_import_retail_db/categories \
 --append 

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/categories
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/categories/part*

Note:
-> --append option will create new files in existing --target-dir 
-> Value of condition in --where clause should be referenced from last run  

=================================================================================================================
#### Problem Statement - Import "products" table incrementally from MySQL to HDFS (By using incremental clause)
=================================================================================================================

-------------
First time :
-------------

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table products \
 --check-column product_id \
 --incremental append \
 --last-value 0 \
 --target-dir /user/cloudera/sqoop_import_retail_db/products

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/products
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/products/part*

----------------------------------
Insert new records into MySQL DB :
----------------------------------

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> use retail_db;
mysql> insert into products values (1346, 59, 'testing 1346', 'testing 1346 - desc', 100, 'http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy');
mysql> insert into products values (1347, 59, 'testing 1347', 'testing 1347 - desc', 100, 'http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy');

-------------
Second time :
-------------

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table products \
 --check-column product_id \
 --incremental append \
 --last-value 1345 \
 --target-dir /user/cloudera/sqoop_import_retail_db/products

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/products
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/products/part*

Note:
-> Following values are supported in --incremental option
   1) --incremental append : for numeric field
   2) --incremental lastmodified : for date field
-> value present in --last-value should be provided from last run 

================================================================================================
#### Problem Statement - Import "orders" table from MySQL to HDFS (By using sequence file format)
================================================================================================

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table orders \
 --target-dir /user/cloudera/sqoop_import_retail_db/orders \
 --as-sequencefile

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/orders

================================================================================================
#### Problem Statement - Import "order_items" table from MySQL to HDFS (By using avro file format)
================================================================================================

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table order_items \
 --target-dir /user/cloudera/sqoop_import_retail_db/order_items \
 --as-avrodatafile

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/order_items
avro-tools tojson hdfs://quickstart.cloudera/user/cloudera/sqoop_import_retail_db/order_items/part-m-00000.avro

/********************************* Import specific table - END **********************************/



/********************************* Import specific table into Hive - START **********************************/

hadoop fs -mkdir /user/cloudera/sqoop_import_retail_db_hive_tmp

hive
hive> create database sqoop_import_retail_db;

=======================================================================
#### Problem Statement - Import "departments" table from MySQL to Hive 
=======================================================================

hive
hive> use sqoop_import_retail_db;
hive> create table departments (department_id int, department_name string);

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table departments \
 --target-dir /user/cloudera/sqoop_import_retail_db_hive_tmp/departments \
 --hive-import \
 --hive-home /user/hive/warehouse \
 --hive-database sqoop_import_retail_db \
 --hive-table departments

hive
hive> use sqoop_import_retail_db;
hive> select * from departments;

===================================================================================================================
#### Problem Statement - Import "categories" table from MySQL to Hive (With custom field delimiter in hive table)
===================================================================================================================

hive
hive> use sqoop_import_retail_db;
hive> create table categories (category_id int, department_id int, category_name string) row format delimited fields terminated by "," stored as textfile;

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table categories \
 --target-dir /user/cloudera/sqoop_import_retail_db_hive_tmp/categories \
 --fields-terminated-by "," \
 --hive-import \
 --hive-home /user/hive/warehouse \
 --hive-database sqoop_import_retail_db \
 --hive-table categories

hive
hive> use sqoop_import_retail_db;
hive> select * from categories;

===================================================================================================================
#### Problem Statement - Import "customers" table from MySQL to Hive (With sequence file format in hive table)
===================================================================================================================


hive
hive> use sqoop_import_retail_db;
hive> create table customers (customer_id int, customer_fname string, customer_lname string, customer_email string, customer_password string, customer_street string, customer_city string, customer_state string, customer_zipcode string) stored as sequencefile;

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table customers \
 --target-dir /user/hive/warehouse/sqoop_import_retail_db.db/customers \
 --append \
 --as-sequencefile 

Note:
-> --as-sequencefile is not working from hive

===================================================================================================================
#### Problem Statement - Import "products" table from MySQL to Hive (With avro file format in hive table)
===================================================================================================================

avro-tools getschema  hdfs://quickstart.cloudera/user/cloudera/sqoop_import_all_retail_db_avro_comp/products/part-m-00000.avro > products.avsc
hadoop fs -mkdir /user/cloudera/avro_schema
hadoop fs -put /home/cloudera/products.avsc /user/cloudera/avro_schema
hadoop fs -cat /user/cloudera/avro_schema/products.avsc

hive
hive>create table products stored as avro tblproperties ('avro.schema.url'='hdfs://quickstart.cloudera/user/cloudera/avro_schema/products.avsc');

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table products \
 --target-dir /user/hive/warehouse/sqoop_import_retail_db.db/products \
 --append \
 --as-avrodatafile

hadoop fs -ls  /user/hive/warehouse/sqoop_import_retail_db.db/products/
hadoop fs -cat /user/hive/warehouse/sqoop_import_retail_db.db/products/part*
avro-tools tojson hdfs://quickstart.cloudera/user/hive/warehouse/sqoop_import_retail_db.db/products/part-m-00000.avro

hive
hive> use sqoop_import_retail_db;
hive> select * from products;

Note:
-> In this example we are not using hive specific parameter in sqoop command
-> Make sure to use same properties required by hive table i.e. hdfs path, file format etc.

===================================================================================================================
#### Problem Statement - Import "orders" table from MySQL to Hive (With parquet file format in hive table)
===================================================================================================================

hive
hive>create table orders (order_id int, order_date bigint, order_customer_id int, order_status string) stored as PARQUET;

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table orders \
 --target-dir /user/hive/warehouse/sqoop_import_retail_db.db/orders \
 --append \
 --as-parquetfile

hadoop fs -ls  /user/hive/warehouse/sqoop_import_retail_db.db/orders/
parquet-tools meta hdfs://quickstart.cloudera/user/hive/warehouse/sqoop_import_retail_db.db/orders/15208295-5d7a-43fc-b519-8fd50fadf457.parquet
parquet-tools schema hdfs://quickstart.cloudera/user/hive/warehouse/sqoop_import_retail_db.db/orders/15208295-5d7a-43fc-b519-8fd50fadf457.parquet
parquet-tools cat --json hdfs://quickstart.cloudera/user/hive/warehouse/sqoop_import_retail_db.db/orders/15208295-5d7a-43fc-b519-8fd50fadf457.parquet


/********************************* Import specific table into Hive - END **********************************/



/********************************* Export specific table from HDFS to MySQL - START **********************************/

hadoop fs -mkdir /user/cloudera/sqoop_export 

======================================================================================================================================
#### Problem Statement - Export to "departments_tmp_export" table from HDFS to MySQL (With text file format and required delimiters)
======================================================================================================================================

%%%%%%%%%%%% Sqoop - Import %%%%%%%%%%%%%

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> show databases;
mysql> use retail_db;
mysql> create table departments_tmp(department_id int, department_name varchar(50));
mysql> describe departments_tmp;
+-----------------+-------------+------+-----+---------+-------+
| Field           | Type        | Null | Key | Default | Extra |
+-----------------+-------------+------+-----+---------+-------+
| department_id   | int(11)     | YES  |     | NULL    |       |
| department_name | varchar(50) | YES  |     | NULL    |       |
+-----------------+-------------+------+-----+---------+-------+
mysql> insert into departments_tmp values (2, "fitness");
mysql> insert into departments_tmp values (3, "footware");
mysql> insert into departments_tmp values (4, "apperal");
mysql> insert into departments_tmp values (5, "golf");
mysql> insert into departments_tmp values (6, "outdoors");
mysql> insert into departments_tmp values (7, "fan shops");
mysql> insert into departments_tmp values (null, null);

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table departments_tmp \
 --num-mappers 1 \
 --target-dir /user/cloudera/sqoop_export/departments_tmp \
 --fields-terminated-by "|" \
 --enclosed-by "\"" \
 --lines-terminated-by "\n" \
 --null-string "NVL" \
 --null-non-string "-1" \
 --as-textfile

hadoop fs -cat /user/cloudera/sqoop_export/departments_tmp/part*
hadoop fs -cat /user/cloudera/sqoop_export/departments_tmp/part* | wc -l

%%%%%%%%%%%% Sqoop - Export %%%%%%%%%%%%%

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> show databases;
mysql> use retail_db;
mysql> create table departments_tmp_export like departments_tmp;
mysql> describe departments_tmp_export;
+-----------------+-------------+------+-----+---------+-------+
| Field           | Type        | Null | Key | Default | Extra |
+-----------------+-------------+------+-----+---------+-------+
| department_id   | int(11)     | YES  |     | NULL    |       |
| department_name | varchar(50) | YES  |     | NULL    |       |
+-----------------+-------------+------+-----+---------+-------+

sqoop export \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table departments_tmp_export \
 --export-dir /user/cloudera/sqoop_export/departments_tmp \
 --num-mappers 1 \
 --input-fields-terminated-by "|" \
 --input-enclosed-by "\"" \
 --input-lines-terminated-by "\n" \
 --input-null-string "NVL" \
 --input-null-non-string "-1"

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> show databases;
mysql> use retail_db;
mysql> select * from departments_tmp_export;

=====
Note:
=====
-> While exporting data from HDFS to RDBMS below steps will get executed
   1) Number of blocks will be find out from file available under --export-dir
   2) Block will be devided into number of mappers
-> --input-fields-terminated-by option in export is counter part of --fields-terminated-by in import
-> --input-enclosed-by option in export is counter part of --enclosed-by in import
-> --input-lines-terminated-by option in export is counter part of --lines-terminated-by in import
-> --input-null-string option in export is counter part of --null-string in import
-> --input-null-non-string option in export is counter part of --null-non-string in import

==================================================================================================================================
#### Problem Statement - Export to "categories_export" table from HDFS to MySQL (With text file format and upsert/merge feature)
==================================================================================================================================

%%%%%%%%%%%% Sqoop - Import %%%%%%%%%%%%%

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> show databases;
mysql> use retail_db;
mysql> describe categories;
+------------------------+-------------+------+-----+---------+----------------+
| Field                  | Type        | Null | Key | Default | Extra          |
+------------------------+-------------+------+-----+---------+----------------+
| category_id            | int(11)     | NO   | PRI | NULL    | auto_increment |
| category_department_id | int(11)     | NO   |     | NULL    |                |
| category_name          | varchar(45) | NO   |     | NULL    |                |
+------------------------+-------------+------+-----+---------+----------------+

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table categories \
 --target-dir /user/cloudera/sqoop_export/categories \
 --as-textfile

hadoop fs -cat /user/cloudera/sqoop_export/categories/part*
hadoop fs -cat /user/cloudera/sqoop_export/categories/part* | wc -l

%%%%%%%%%%%% Sqoop - Export %%%%%%%%%%%%%

Update HDFS file located at hdfs://quickstart.cloudera/user/cloudera/sqoop_export/categories/part-m-00003

59,7,testing - HDFS Update
60,7,testing - 60 - HDFS Update
61,7,HDFS Insert

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> show databases;
mysql> use retail_db;
mysql> create table categories_export like categories;
mysql> describe categories_export;
+------------------------+-------------+------+-----+---------+----------------+
| Field                  | Type        | Null | Key | Default | Extra          |
+------------------------+-------------+------+-----+---------+----------------+
| category_id            | int(11)     | NO   | PRI | NULL    | auto_increment |
| category_department_id | int(11)     | NO   |     | NULL    |                |
| category_name          | varchar(45) | NO   |     | NULL    |                |
+------------------------+-------------+------+-----+---------+----------------+

sqoop export \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table categories_export \
 --export-dir /user/cloudera/sqoop_export/categories \
 --update-key category_id \
 --update-mode allowinsert \
 --batch

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> show databases;
mysql> use retail_db;
mysql> select * from categories_export;

=====
Note:
=====
-> Possible available options under --update-mode are:
   1) updateonly - this should update data in table while exporting data (Execute update DML statement based on --update-key column in RDBMS)
   2) allowinsert - this should insert new data apart from updating data while exporting data (Execute insert/update DML statement in RDBMS) 

=================================================================================================================================
#### Problem Statement - Export to "product_export" table from HDFS to MySQL (With avro file format and all or nothing semantic)
=================================================================================================================================

%%%%%%%%%%%% Sqoop - Import %%%%%%%%%%%%%

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> show databases;
mysql> use retail_db;
mysql> describe products;
+---------------------+--------------+------+-----+---------+----------------+
| Field               | Type         | Null | Key | Default | Extra          |
+---------------------+--------------+------+-----+---------+----------------+
| product_id          | int(11)      | NO   | PRI | NULL    | auto_increment |
| product_category_id | int(11)      | NO   |     | NULL    |                |
| product_name        | varchar(45)  | NO   |     | NULL    |                |
| product_description | varchar(255) | NO   |     | NULL    |                |
| product_price       | float        | NO   |     | NULL    |                |
| product_image       | varchar(255) | NO   |     | NULL    |                |
+---------------------+--------------+------+-----+---------+----------------+

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table products \
 --target-dir /user/cloudera/sqoop_export/products \
 --as-avrodatafile

hadoop fs -ls /user/cloudera/sqoop_export/products
hadoop fs -cat /user/cloudera/sqoop_export/products/part*

%%%%%%%%%%%% Sqoop - Export %%%%%%%%%%%%%

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> show databases;
mysql> use retail_db;
mysql> create table products_export like products;
mysql> describe products_export;
+---------------------+--------------+------+-----+---------+----------------+
| Field               | Type         | Null | Key | Default | Extra          |
+---------------------+--------------+------+-----+---------+----------------+
| product_id          | int(11)      | NO   | PRI | NULL    | auto_increment |
| product_category_id | int(11)      | NO   |     | NULL    |                |
| product_name        | varchar(45)  | NO   |     | NULL    |                |
| product_description | varchar(255) | NO   |     | NULL    |                |
| product_price       | float        | NO   |     | NULL    |                |
| product_image       | varchar(255) | NO   |     | NULL    |                |
+---------------------+--------------+------+-----+---------+----------------+
mysql> create table products_stage_export like products;
mysql> describe products_stage_export;
+---------------------+--------------+------+-----+---------+----------------+
| Field               | Type         | Null | Key | Default | Extra          |
+---------------------+--------------+------+-----+---------+----------------+
| product_id          | int(11)      | NO   | PRI | NULL    | auto_increment |
| product_category_id | int(11)      | NO   |     | NULL    |                |
| product_name        | varchar(45)  | NO   |     | NULL    |                |
| product_description | varchar(255) | NO   |     | NULL    |                |
| product_price       | float        | NO   |     | NULL    |                |
| product_image       | varchar(255) | NO   |     | NULL    |                |
+---------------------+--------------+------+-----+---------+----------------+

sqoop export \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table products_export \
 --export-dir /user/cloudera/sqoop_export/products \
 --staging-table products_stage_export \
 --clear-staging-table \
 --batch

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> show databases;
mysql> use retail_db;
mysql> select * from products_stage_export;
mysql> select * from products_export;


=====
Note:
=====
-> --staging-table structure should always be same as --table
-> --clear-staging-table option will truncate the table first before exporting data
-> This will usefull to perform all or nothing semantic i.e. in case of any issue occurs in either of the mapper, it will not copy data from staging table to actual table

/********************************* Export specific table from HDFS to MySQL - END **********************************/

