
sqoop import --help

/********************************* Import specific table - START **********************************/

hadoop fs -mkdir /user/cloudera/sqoop_import_retail_db

========================================================================================
#### Problem Statement - Import "departments" table from MySQL to HDFS (Default import)
========================================================================================

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table departments \
 --target-dir /user/cloudera/sqoop_import_retail_db/departments

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/departments
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/departments/part*

Note:
-> --num-mappers default value is 4

==================================================================================================================
#### Problem Statement - Import "customers" table from MySQL to HDFS (By using boundary query and columns option)
==================================================================================================================

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table customers \
 --columns "customer_id, customer_fname, customer_lname, customer_email, customer_password, customer_street, customer_city, customer_state, customer_zipcode" \
 --boundary-query "select min(customer_id), max(customer_id) from customers" \
 --target-dir /user/cloudera/sqoop_import_retail_db/customers \
 --num-mappers 1 

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/customers
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/customers/part*

========================================================================================================
#### Problem Statement - Import "departments_nopk" table from MySQL to HDFS (By using split by option)
========================================================================================================

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> use retail_db;
mysql> create table departments_nopk as select * from departments;
mysql> describe departments_nopk;

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table departments_nopk \
 --split-by department_id \
 --target-dir /user/cloudera/sqoop_import_retail_db/departments_nopk

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/departments_nopk
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/departments_nopk/part*

Note:
-> --split-by option is useful in case of table doesn't have primary key
-> --num-mappers=1 can be use in absense on --split-by option

==========================================================================================================
#### Problem Statement - Import data using query from MySQL to HDFS (By using query and splity by option)
==========================================================================================================

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --query "select * from orders join order_items on (orders.order_id = order_items.order_item_order_id) where \$CONDITIONS" \
 --split-by order_id \
 --target-dir /user/cloudera/sqoop_import_retail_db/order_join

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/order_join
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/order_join/part*

Note:
-> --table and --query options are mutually exclusive
-> where \$CONDITIONS is mandatory in case of using --query option
-> --split-by option is useful in case of using --query option
-> --num-mappers=1 can be use in absense on --split-by option

=======================================================================================================================
#### Problem Statement - Import "categories" table incrementally from MySQL to HDFS (By using where and append clause)
=======================================================================================================================

-------------
First time :
-------------

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table categories \
 --where "category_id > 0" \
 --append \
 --target-dir /user/cloudera/sqoop_import_retail_db/categories

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/categories
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/categories/part*

----------------------------------
Insert new records into MySQL DB :
----------------------------------

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> use retail_db;
mysql> insert into categories values (59, 7, 'testing 59');
mysql> insert into categories values (60, 7, 'testing 60');

-------------
Second time :
-------------

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table categories \
 --where "category_id > 58" \
 --target-dir /user/cloudera/sqoop_import_retail_db/categories \
 --append 

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/categories
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/categories/part*

Note:
-> --append option will create new files in existing --target-dir 
-> Value of condition in --where clause should be referenced from last run  

=================================================================================================================
#### Problem Statement - Import "products" table incrementally from MySQL to HDFS (By using incremental clause)
=================================================================================================================

-------------
First time :
-------------

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table products \
 --check-column product_id \
 --incremental append \
 --last-value 0 \
 --target-dir /user/cloudera/sqoop_import_retail_db/products

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/products
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/products/part*

----------------------------------
Insert new records into MySQL DB :
----------------------------------

mysql -u retail_dba -p -h quickstart.cloudera -P 3306
mysql> use retail_db;
mysql> insert into products values (1346, 59, 'testing 1346', 'testing 1346 - desc', 100, 'http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy');
mysql> insert into products values (1347, 59, 'testing 1347', 'testing 1347 - desc', 100, 'http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy');

-------------
Second time :
-------------

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table products \
 --check-column product_id \
 --incremental append \
 --last-value 1345 \
 --target-dir /user/cloudera/sqoop_import_retail_db/products

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/products
hadoop fs -cat /user/cloudera/sqoop_import_retail_db/products/part*

Note:
-> Following values are supported in --incremental option
   1) --incremental append : for numeric field
   2) --incremental lastmodified : for date field
-> value present in --last-value should be provided from last run 

================================================================================================
#### Problem Statement - Import "orders" table from MySQL to HDFS (By using sequence file format)
================================================================================================

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table orders \
 --target-dir /user/cloudera/sqoop_import_retail_db/orders \
 --as-sequencefile

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/orders

================================================================================================
#### Problem Statement - Import "order_items" table from MySQL to HDFS (By using avro file format)
================================================================================================

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table order_items \
 --target-dir /user/cloudera/sqoop_import_retail_db/order_items \
 --as-avrodatafile

hadoop fs -ls -R /user/cloudera/sqoop_import_retail_db/order_items
avro-tools tojson hdfs://quickstart.cloudera/user/cloudera/sqoop_import_retail_db/order_items/part-m-00000.avro

/********************************* Import specific table - END **********************************/



/********************************* Import specific table into Hive - START **********************************/

hadoop fs -mkdir /user/cloudera/sqoop_import_retail_db_hive_tmp

hive
hive> create database sqoop_import_retail_db;

=======================================================================
#### Problem Statement - Import "departments" table from MySQL to Hive 
=======================================================================

hive
hive> use sqoop_import_retail_db;
hive> create table departments (department_id int, department_name string);

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table departments \
 --target-dir /user/cloudera/sqoop_import_retail_db_hive_tmp/departments \
 --hive-import \
 --hive-home /user/hive/warehouse \
 --hive-database sqoop_import_retail_db \
 --hive-table departments

hive
hive> use sqoop_import_retail_db;
hive> select * from departments;

===================================================================================================================
#### Problem Statement - Import "categories" table from MySQL to Hive (With custom field delimiter in hive table)
===================================================================================================================

hive
hive> use sqoop_import_retail_db;
hive> create table categories (category_id int, department_id int, category_name string) 
row format delimited 
fields terminated by "," 
stored as textfile;

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table categories \
 --target-dir /user/cloudera/sqoop_import_retail_db_hive_tmp/categories \
 --fields-terminated-by "," \
 --hive-import \
 --hive-home /user/hive/warehouse \
 --hive-database sqoop_import_retail_db \
 --hive-table categories

hive
hive> use sqoop_import_retail_db;
hive> select * from categories;

===================================================================================================================
#### Problem Statement - Import "customers" table from MySQL to Hive (With sequence file format in hive table)
===================================================================================================================


hive
hive> use sqoop_import_retail_db;
hive> create table customers (
customer_id int, 
customer_fname string, 
customer_lname string, 
customer_email string, 
customer_password string, 
customer_street string, 
customer_city string, 
customer_state string, 
customer_zipcode string) 
stored as sequencefile;

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table customers \
 --target-dir /user/hive/warehouse/sqoop_import_retail_db.db/customers \
 --append \
 --as-sequencefile 

Note:
-> --as-sequencefile is not working from hive

===================================================================================================================
#### Problem Statement - Import "products" table from MySQL to Hive (With avro file format in hive table)
===================================================================================================================

avro-tools getschema  hdfs://quickstart.cloudera/user/cloudera/sqoop_import_all_retail_db_avro_comp/products/part-m-00000.avro > products.avsc
hadoop fs -mkdir /user/cloudera/avro_schema
hadoop fs -put /home/cloudera/products.avsc /user/cloudera/avro_schema
hadoop fs -cat /user/cloudera/avro_schema/products.avsc

hive
hive>create table products 
stored as avro 
tblproperties ('avro.schema.url'='hdfs://quickstart.cloudera/user/cloudera/avro_schema/products.avsc');

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table products \
 --target-dir /user/hive/warehouse/sqoop_import_retail_db.db/products \
 --append \
 --as-avrodatafile

hadoop fs -ls  /user/hive/warehouse/sqoop_import_retail_db.db/products/
hadoop fs -cat /user/hive/warehouse/sqoop_import_retail_db.db/products/part*
avro-tools tojson hdfs://quickstart.cloudera/user/hive/warehouse/sqoop_import_retail_db.db/products/part-m-00000.avro

hive
hive> use sqoop_import_retail_db;
hive> select * from products;

Note:
-> In this example we are not using hive specific parameter in sqoop command
-> Make sure to use same properties required by hive table i.e. hdfs path, file format etc.

===================================================================================================================
#### Problem Statement - Import "orders" table from MySQL to Hive (With parquet file format in hive table)
===================================================================================================================

hive
hive>create table orders (
order_id int, 
order_date bigint, 
order_customer_id int, 
order_status string) 
stored as PARQUET;

sqoop import \
 --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
 --username retail_dba \
 --password cloudera \
 --table orders \
 --target-dir /user/hive/warehouse/sqoop_import_retail_db.db/orders \
 --append \
 --as-parquetfile

hadoop fs -ls  /user/hive/warehouse/sqoop_import_retail_db.db/orders/
parquet-tools meta hdfs://quickstart.cloudera/user/hive/warehouse/sqoop_import_retail_db.db/orders/15208295-5d7a-43fc-b519-8fd50fadf457.parquet
parquet-tools schema hdfs://quickstart.cloudera/user/hive/warehouse/sqoop_import_retail_db.db/orders/15208295-5d7a-43fc-b519-8fd50fadf457.parquet
parquet-tools cat --json hdfs://quickstart.cloudera/user/hive/warehouse/sqoop_import_retail_db.db/orders/15208295-5d7a-43fc-b519-8fd50fadf457.parquet


/********************************* Import specific table into Hive - END **********************************/




